# Extracted Content from arxiv.org

**Source:** https://arxiv.org/abs/2310.06825
**Crawled:** 2026-02-02T05:29:38.966Z
**Pages:** 1 extracted, 0 skipped
**Tokens:** ~610 estimated
**Duration:** 3s

---

## Mistral 7B

> Source: https://arxiv.org/abs/2310.06825
> Tokens: ~610

## Title:Mistral 7B

Authors:[Albert Q. Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+A+Q), [Alexandre Sablayrolles](https://arxiv.org/search/cs?searchtype=author&query=Sablayrolles,+A), [Arthur Mensch](https://arxiv.org/search/cs?searchtype=author&query=Mensch,+A), [Chris Bamford](https://arxiv.org/search/cs?searchtype=author&query=Bamford,+C), [Devendra Singh Chaplot](https://arxiv.org/search/cs?searchtype=author&query=Chaplot,+D+S), [Diego de las Casas](https://arxiv.org/search/cs?searchtype=author&query=de+las+Casas,+D), [Florian Bressand](https://arxiv.org/search/cs?searchtype=author&query=Bressand,+F), [Gianna Lengyel](https://arxiv.org/search/cs?searchtype=author&query=Lengyel,+G), [Guillaume Lample](https://arxiv.org/search/cs?searchtype=author&query=Lample,+G), [Lucile Saulnier](https://arxiv.org/search/cs?searchtype=author&query=Saulnier,+L), [Lélio Renard Lavaud](https://arxiv.org/search/cs?searchtype=author&query=Lavaud,+L+R), [Marie-Anne Lachaux](https://arxiv.org/search/cs?searchtype=author&query=Lachaux,+M), [Pierre Stock](https://arxiv.org/search/cs?searchtype=author&query=Stock,+P), [Teven Le Scao](https://arxiv.org/search/cs?searchtype=author&query=Scao,+T+L), [Thibaut Lavril](https://arxiv.org/search/cs?searchtype=author&query=Lavril,+T), [Thomas Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+T), [Timothée Lacroix](https://arxiv.org/search/cs?searchtype=author&query=Lacroix,+T), [William El Sayed](https://arxiv.org/search/cs?searchtype=author&query=Sayed,+W+E)

[View PDF](https://arxiv.org/pdf/2310.06825)

> Abstract:We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.

## Submission history

From: Devendra Singh Chaplot \[[view email](https://arxiv.org/show-email/1509be8a/2310.06825)\]
**\[v1\]** Tue, 10 Oct 2023 17:54:58 UTC (2,241 KB)