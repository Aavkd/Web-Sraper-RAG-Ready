{
	"source": "https://crawlee.dev/docs/introduction",
	"crawledAt": "2026-02-02T05:20:13.883Z",
	"summary": {
		"pagesAttempted": 11,
		"pagesExtracted": 5,
		"pagesSkipped": 6,
		"totalTokensEstimate": 5955,
		"startTime": "2026-02-02T05:20:08.321Z",
		"endTime": "2026-02-02T05:20:13.872Z"
	},
	"pages": [
		{
			"url": "https://crawlee.dev/docs/introduction",
			"title": "Introduction",
			"markdown": "- - Introduction\n\nVersion: 3.15\n\n## Introduction\n\nCrawlee covers your crawling and scraping end-to-end and helps you **build reliable scrapers. Fast.**\n\nYour crawlers will appear human-like and fly under the radar of modern bot protections even with the default configuration. Crawlee gives you the tools to crawl the web for links, scrape data and persistently store it in machine-readable formats, without having to worry about the technical details. And thanks to rich configuration options, you can tweak almost any aspect of Crawlee to suit your project's needs if the default settings don't cut it.\n\n### What you will learn\n\nThe goal of the introduction is to provide a step-by-step guide to the most important features of Crawlee. It will walk you through creating the simplest of crawlers that only prints text to console, all the way up to a full-featured scraper that collects links from a website and extracts data.\n\n###  Features\n\n- Single interface for **HTTP and headless browser**crawling- Persistent **queue**for URLs to crawl (breadth & depth first)- Pluggable **storage**of both tabular data and files- Automatic **scaling**with available system resources- Integrated **proxy rotation**and session management- Lifecycles customizable with **hooks**- **CLI**to bootstrap your projects- Configurable **routing**, **error handling**and **retries**- **Dockerfiles**ready to deploy- Written in **TypeScript**with generics\n\n####  HTTP crawling\n\n- Zero config **HTTP2 support**, even for proxies- Automatic generation of **browser-like headers**- Replication of browser **TLS fingerprints**- Integrated fast **HTML parsers**. Cheerio and JSDOM- Yes, you can scrape **JSON APIs**as well\n\n####  Real browser crawling\n\n- JavaScript **rendering**and **screenshots**- **Headless**and **headful**support- Zero-config generation of **human-like fingerprints**- Automatic **browser management**- Use **Playwright**and **Puppeteer**with the same interface- **Chrome**, **Firefox**, **Webkit**and many others\n\n### Next steps\n\nNext, you will install Crawlee and learn how to bootstrap projects with the Crawlee CLI.\n\n[Edit this page](https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.15/introduction/index.mdx)\n\nLast updated on **Jan 30, 2026** by **nikitachapovskii-dev**",
			"chunks": [
				{
					"id": "docs_introduction_chunk_1",
					"content": "- - Introduction\n\nVersion: 3.15",
					"tokensEstimate": 8,
					"sourceUrl": "https://crawlee.dev/docs/introduction",
					"pageTitle": "Introduction"
				},
				{
					"id": "docs_introduction_chunk_2",
					"content": "## Introduction\n\nCrawlee covers your crawling and scraping end-to-end and helps you **build reliable scrapers. Fast.**\n\nYour crawlers will appear human-like and fly under the radar of modern bot protections even with the default configuration. Crawlee gives you the tools to crawl the web for links, scrape data and persistently store it in machine-readable formats, without having to worry about the technical details. And thanks to rich configuration options, you can tweak almost any aspect of Crawlee to suit your project's needs if the default settings don't cut it.\n\n### What you will learn\n\nThe goal of the introduction is to provide a step-by-step guide to the most important features of Crawlee. It will walk you through creating the simplest of crawlers that only prints text to console, all the way up to a full-featured scraper that collects links from a website and extracts data.",
					"tokensEstimate": 223,
					"sourceUrl": "https://crawlee.dev/docs/introduction",
					"pageTitle": "Introduction"
				},
				{
					"id": "docs_introduction_chunk_3",
					"content": "###  Features\n\n- Single interface for **HTTP and headless browser**crawling- Persistent **queue**for URLs to crawl (breadth & depth first)- Pluggable **storage**of both tabular data and files- Automatic **scaling**with available system resources- Integrated **proxy rotation**and session management- Lifecycles customizable with **hooks**- **CLI**to bootstrap your projects- Configurable **routing**, **error handling**and **retries**- **Dockerfiles**ready to deploy- Written in **TypeScript**with generics\n\n####  HTTP crawling\n\n- Zero config **HTTP2 support**, even for proxies- Automatic generation of **browser-like headers**- Replication of browser **TLS fingerprints**- Integrated fast **HTML parsers**. Cheerio and JSDOM- Yes, you can scrape **JSON APIs**as well\n\n####  Real browser crawling\n\n- JavaScript **rendering**and **screenshots**- **Headless**and **headful**support- Zero-config generation of **human-like fingerprints**- Automatic **browser management**- Use **Playwright**and **Puppeteer**with the same interface- **Chrome**, **Firefox**, **Webkit**and many others\n\n### Next steps\n\nNext, you will install Crawlee and learn how to bootstrap projects with the Crawlee CLI.\n\n[Edit this page](https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.15/introduction/index.mdx)\n\nLast updated on **Jan 30, 2026** by **nikitachapovskii-dev**",
					"tokensEstimate": 344,
					"sourceUrl": "https://crawlee.dev/docs/introduction",
					"pageTitle": "Introduction"
				}
			],
			"metadata": {
				"url": "https://crawlee.dev/docs/introduction",
				"title": "Introduction",
				"depth": 0,
				"crawledAt": "2026-02-02T05:20:11.315Z",
				"tokensEstimate": 574
			}
		},
		{
			"url": "https://crawlee.dev/python",
			"title": "Crawlee for Python 路 Fast, reliable Python web crawlers.",
			"markdown": "## Build reliable web scrapers. Fast.\n\nCrawlee is a web scraping library for JavaScript and Python. It handles blocking, crawling, proxies, and browsers for you.\n\n![Crawlee Python](https://crawlee.dev/img/crawlee-python-light.svg)![Crawlee Python](https://crawlee.dev/img/crawlee-python-dark.svg)\n\n[Run on](https://console.apify.com/actors/HH9rhkFXiZbheuq1V?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.eyJpbnB1dCI6IntcImNvZGVcIjpcImltcG9ydCBhc3luY2lvXFxuXFxuZnJvbSBjcmF3bGVlLmNyYXdsZXJzIGltcG9ydCBQbGF5d3JpZ2h0Q3Jhd2xlciwgUGxheXdyaWdodENyYXdsaW5nQ29udGV4dFxcblxcblxcbmFzeW5jIGRlZiBtYWluKCkgLT4gTm9uZTpcXG4gICAgY3Jhd2xlciA9IFBsYXl3cmlnaHRDcmF3bGVyKFxcbiAgICAgICAgbWF4X3JlcXVlc3RzX3Blcl9jcmF3bD0xMCwgICMgTGltaXQgdGhlIG1heCByZXF1ZXN0cyBwZXIgY3Jhd2wuXFxuICAgICAgICBoZWFkbGVzcz1UcnVlLCAgIyBSdW4gaW4gaGVhZGxlc3MgbW9kZSAoc2V0IHRvIEZhbHNlIHRvIHNlZSB0aGUgYnJvd3NlcikuXFxuICAgICAgICBicm93c2VyX3R5cGU9J2ZpcmVmb3gnLCAgIyBVc2UgRmlyZWZveCBicm93c2VyLlxcbiAgICApXFxuXFxuICAgICMgRGVmaW5lIHRoZSBkZWZhdWx0IHJlcXVlc3QgaGFuZGxlciwgd2hpY2ggd2lsbCBiZSBjYWxsZWQgZm9yIGV2ZXJ5IHJlcXVlc3QuXFxuICAgIEBjcmF3bGVyLnJvdXRlci5kZWZhdWx0X2hhbmRsZXJcXG4gICAgYXN5bmMgZGVmIHJlcXVlc3RfaGFuZGxlcihjb250ZXh0OiBQbGF5d3JpZ2h0Q3Jhd2xpbmdDb250ZXh0KSAtPiBOb25lOlxcbiAgICAgICAgY29udGV4dC5sb2cuaW5mbyhmJ1Byb2Nlc3Npbmcge2NvbnRleHQucmVxdWVzdC51cmx9IC4uLicpXFxuXFxuICAgICAgICAjIEV4dHJhY3QgZGF0YSBmcm9tIHRoZSBwYWdlIHVzaW5nIFBsYXl3cmlnaHQgQVBJLlxcbiAgICAgICAgZGF0YSA9IHtcXG4gICAgICAgICAgICAndXJsJzogY29udGV4dC5yZXF1ZXN0LnVybCxcXG4gICAgICAgICAgICAndGl0bGUnOiBhd2FpdCBjb250ZXh0LnBhZ2UudGl0bGUoKSxcXG4gICAgICAgIH1cXG5cXG4gICAgICAgICMgUHVzaCB0aGUgZXh0cmFjdGVkIGRhdGEgdG8gdGhlIGRlZmF1bHQgZGF0YXNldC5cXG4gICAgICAgIGF3YWl0IGNvbnRleHQucHVzaF9kYXRhKGRhdGEpXFxuXFxuICAgICAgICAjIEV4dHJhY3QgYWxsIGxpbmtzIG9uIHRoZSBwYWdlIGFuZCBlbnF1ZXVlIHRoZW0uXFxuICAgICAgICBhd2FpdCBjb250ZXh0LmVucXVldWVfbGlua3MoKVxcblxcbiAgICAjIFJ1biB0aGUgY3Jhd2xlciB3aXRoIHRoZSBpbml0aWFsIGxpc3Qgb2YgVVJMcy5cXG4gICAgYXdhaXQgY3Jhd2xlci5ydW4oWydodHRwczovL2NyYXdsZWUuZGV2J10pXFxuXFxuICAgICMgRXhwb3J0IHRoZSBlbnRpcmUgZGF0YXNldCB0byBhIENTViBmaWxlLlxcbiAgICBhd2FpdCBjcmF3bGVyLmV4cG9ydF9kYXRhKCdyZXN1bHRzLmNzdicpXFxuXFxuICAgICMgT3IgYWNjZXNzIHRoZSBkYXRhIGRpcmVjdGx5LlxcbiAgICBkYXRhID0gYXdhaXQgY3Jhd2xlci5nZXRfZGF0YSgpXFxuICAgIGNyYXdsZXIubG9nLmluZm8oZidFeHRyYWN0ZWQgZGF0YToge2RhdGEuaXRlbXN9JylcXG5cXG5cXG5pZiBfX25hbWVfXyA9PSAnX19tYWluX18nOlxcbiAgICBhc3luY2lvLnJ1bihtYWluKCkpXFxuXCJ9Iiwib3B0aW9ucyI6eyJidWlsZCI6ImxhdGVzdCIsImNvbnRlbnRUeXBlIjoiYXBwbGljYXRpb24vanNvbjsgY2hhcnNldD11dGYtOCIsIm1lbW9yeSI6NDA5NiwidGltZW91dCI6MTgwfX0.xkqQZRAs2ksZSltV4qM1so0YP8n-i-KMFQkUnLsqJN4&asrc=run_on_apify)\n\n```python\nimport asynciofrom crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContextasync def main() -> None: crawler = PlaywrightCrawler( max_requests_per_crawl=10, # Limit the max requests per crawl. headless=True, # Run in headless mode (set to False to see the browser). browser_type='firefox', # Use Firefox browser. ) # Define the default request handler, which will be called for every request. @crawler.router.default_handler async def request_handler(context: PlaywrightCrawlingContext) -> None: context.log.info(f'Processing {context.request.url} ...') # Extract data from the page using Playwright API. data = { 'url': context.request.url, 'title': await context.page.title(), } # Push the extracted data to the default dataset. await context.push_data(data) # Extract all links on the page and enqueue them. await context.enqueue_links() # Run the crawler with the initial list of URLs. await crawler.run(['https://crawlee.dev']) # Export the entire dataset to a CSV file. await crawler.export_data('results.csv') # Or access the data directly. data = await crawler.get_data() crawler.log.info(f'Extracted data: {data.items}')if __name__ == '__main__': asyncio.run(main())\n```\n\nOr start with a template from our CLI\n\n` $uvx 'crawlee[cli]' create my-crawler `\n\nBuilt with  by Apify. Forever free and open-source.\n\n## What are the benefits?\n\n### Unblock websites by default\n\nCrawlee crawls stealthily with zero configuration, but you can customize its behavior to overcome any protection. Real-world fingerprints included.\n\n[Learn more](https://crawlee.dev/python/docs/guides/avoid-blocking)\n\n```text\nfingerprint_generator = DefaultFingerprintGenerator( header_options=HeaderGeneratorOptions( browsers=['chromium', 'firefox'], devices=['mobile'], locales=['en-US'] ),)\n```\n\n### Work with your favorite tools\n\nCrawlee integrates BeautifulSoup, Cheerio, Puppeteer, Playwright, and other popular open-source tools. No need to learn new syntax.\n\n[Learn more](https://crawlee.dev/python/docs/quick-start#choose-your-crawler)\n\n![Work with your favorite tools](https://crawlee.dev/python/img/favorite-tools-light.webp)![Work with your favorite tools](https://crawlee.dev/python/img/favorite-tools-dark.webp)\n\n### One API for headless and HTTP\n\nSwitch between HTTP and headless without big rewrites thanks to a shared API. Or even let Adaptive crawler decide if JS rendering is needed.\n\n[Learn more](https://crawlee.dev/python/api)\n\n```python\ncrawler = AdaptivePlaywrightCrawler.with_parsel_static_parser()@crawler.router.default_handlerasync def request_handler(context: AdaptivePlaywrightCrawlingContext) -> None: prices = await context.query_selector_all('span.price') await context.enqueue_links()\n```\n\n## What else is in Crawlee?\n\n## Deploy to cloud\n\nCrawlee, by Apify, works anywhere, but Apify offers the best experience. Easily turn your project into an\n\n[Actor](https://apify.com/actors)a serverless micro-app with built-in infra, proxies, and storage.\n\n[Deploy to Apify](https://docs.apify.com/platform/actors/development/deployment)\n\nInstall Apify SDK and Apify CLI.\n\nAdd\n\nActor.init()\n\nto the beginning and\n\nActor.exit()\n\nto the end of your code.\n\nUse the Apify CLI to push the code to the Apify platform.\n\n## Get started now!\n\nCrawlee wont fix broken selectors for you (yet), but it makes building and maintaining reliable crawlers faster and easierso you can focus on what matters most.",
			"chunks": [
				{
					"id": "python_chunk_1",
					"content": "## Build reliable web scrapers. Fast.\n\n\n\nCrawlee is a web scraping library for JavaScript and Python. It handles blocking, crawling, proxies, and browsers for you.\n\n![Crawlee Python](https://crawlee.dev/img/crawlee-python-light.svg)![Crawlee Python](https://crawlee.dev/img/crawlee-python-dark.svg)\n\n[Run on](https://console.apify.com/actors/HH9rhkFXiZbheuq1V?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.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.xkqQZRAs2ksZSltV4qM1so0YP8n-i-KMFQkUnLsqJN4&asrc=run_on_apify)",
					"tokensEstimate": 647,
					"sourceUrl": "https://crawlee.dev/python",
					"pageTitle": "Crawlee for Python 路 Fast, reliable Python web crawlers."
				},
				{
					"id": "python_chunk_2",
					"content": "```python\nimport asynciofrom crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContextasync def main() -> None: crawler = PlaywrightCrawler( max_requests_per_crawl=10, # Limit the max requests per crawl. headless=True, # Run in headless mode (set to False to see the browser). browser_type='firefox', # Use Firefox browser. ) # Define the default request handler, which will be called for every request. @crawler.router.default_handler async def request_handler(context: PlaywrightCrawlingContext) -> None: context.log.info(f'Processing {context.request.url} ...') # Extract data from the page using Playwright API. data = { 'url': context.request.url, 'title': await context.page.title(), } # Push the extracted data to the default dataset. await context.push_data(data) # Extract all links on the page and enqueue them. await context.enqueue_links() # Run the crawler with the initial list of URLs. await crawler.run(['https://crawlee.dev']) # Export the entire dataset to a CSV file. await crawler.export_data('results.csv') # Or access the data directly. data = await crawler.get_data() crawler.log.info(f'Extracted data: {data.items}')if __name__ == '__main__': asyncio.run(main())\n```\n\nOr start with a template from our CLI\n\n` $uvx 'crawlee[cli]' create my-crawler `\n\nBuilt with  by Apify. Forever free and open-source.\n\n## What are the benefits?",
					"tokensEstimate": 341,
					"sourceUrl": "https://crawlee.dev/python",
					"pageTitle": "Crawlee for Python 路 Fast, reliable Python web crawlers."
				},
				{
					"id": "python_chunk_3",
					"content": "### Unblock websites by default\n\nCrawlee crawls stealthily with zero configuration, but you can customize its behavior to overcome any protection. Real-world fingerprints included.\n\n[Learn more](https://crawlee.dev/python/docs/guides/avoid-blocking)\n\n```text\nfingerprint_generator = DefaultFingerprintGenerator( header_options=HeaderGeneratorOptions( browsers=['chromium', 'firefox'], devices=['mobile'], locales=['en-US'] ),)\n```",
					"tokensEstimate": 107,
					"sourceUrl": "https://crawlee.dev/python",
					"pageTitle": "Crawlee for Python 路 Fast, reliable Python web crawlers."
				},
				{
					"id": "python_chunk_4",
					"content": "### Work with your favorite tools\n\nCrawlee integrates BeautifulSoup, Cheerio, Puppeteer, Playwright, and other popular open-source tools. No need to learn new syntax.\n\n[Learn more](https://crawlee.dev/python/docs/quick-start#choose-your-crawler)\n\n![Work with your favorite tools](https://crawlee.dev/python/img/favorite-tools-light.webp)![Work with your favorite tools](https://crawlee.dev/python/img/favorite-tools-dark.webp)",
					"tokensEstimate": 106,
					"sourceUrl": "https://crawlee.dev/python",
					"pageTitle": "Crawlee for Python 路 Fast, reliable Python web crawlers."
				},
				{
					"id": "python_chunk_5",
					"content": "### One API for headless and HTTP\n\nSwitch between HTTP and headless without big rewrites thanks to a shared API. Or even let Adaptive crawler decide if JS rendering is needed.\n\n[Learn more](https://crawlee.dev/python/api)\n\n```python\ncrawler = AdaptivePlaywrightCrawler.with_parsel_static_parser()@crawler.router.default_handlerasync def request_handler(context: AdaptivePlaywrightCrawlingContext) -> None: prices = await context.query_selector_all('span.price') await context.enqueue_links()\n```\n\n## What else is in Crawlee?",
					"tokensEstimate": 130,
					"sourceUrl": "https://crawlee.dev/python",
					"pageTitle": "Crawlee for Python 路 Fast, reliable Python web crawlers."
				},
				{
					"id": "python_chunk_6",
					"content": "## Deploy to cloud\n\nCrawlee, by Apify, works anywhere, but Apify offers the best experience. Easily turn your project into an\n\n[Actor](https://apify.com/actors)a serverless micro-app with built-in infra, proxies, and storage.\n\n[Deploy to Apify](https://docs.apify.com/platform/actors/development/deployment)\n\nInstall Apify SDK and Apify CLI.\n\nAdd\n\nActor.init()\n\nto the beginning and\n\nActor.exit()\n\nto the end of your code.\n\nUse the Apify CLI to push the code to the Apify platform.\n\n## Get started now!\n\nCrawlee wont fix broken selectors for you (yet), but it makes building and maintaining reliable crawlers faster and easierso you can focus on what matters most.",
					"tokensEstimate": 164,
					"sourceUrl": "https://crawlee.dev/python",
					"pageTitle": "Crawlee for Python 路 Fast, reliable Python web crawlers."
				}
			],
			"metadata": {
				"url": "https://crawlee.dev/python",
				"title": "Crawlee for Python 路 Fast, reliable Python web crawlers.",
				"depth": 1,
				"crawledAt": "2026-02-02T05:20:11.979Z",
				"tokensEstimate": 1495
			}
		},
		{
			"url": "https://crawlee.dev/",
			"title": "Crawlee for JavaScript 路 Build reliable crawlers. Fast.",
			"markdown": "## Build reliable web scrapers. Fast.\n\nCrawlee is a web scraping library for JavaScript and Python. It handles blocking, crawling, proxies, and browsers for you.\n\n![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-light.svg)![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-dark.svg)\n\n`npx crawlee create my-crawler`\n\n![Crawlee Python](https://crawlee.dev/img/crawlee-python-light.svg)![Crawlee Python](https://crawlee.dev/img/crawlee-python-dark.svg)\n\n`uvx 'crawlee[cli]' create my-crawler`\n\n[Run on](https://console.apify.com/actors/6i5QsHBMtm3hKph70?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.eyJpbnB1dCI6IntcbiAgICBcImNvZGVcIjogXCJpbXBvcnQgeyBQbGF5d3JpZ2h0Q3Jhd2xlciB9IGZyb20gJ2NyYXdsZWUnO1xcblxcbi8vIENyYXdsZXIgc2V0dXAgZnJvbSB0aGUgcHJldmlvdXMgZXhhbXBsZS5cXG5jb25zdCBjcmF3bGVyID0gbmV3IFBsYXl3cmlnaHRDcmF3bGVyKHtcXG4gICAgLy8gVXNlIHRoZSByZXF1ZXN0SGFuZGxlciB0byBwcm9jZXNzIGVhY2ggb2YgdGhlIGNyYXdsZWQgcGFnZXMuXFxuICAgIGFzeW5jIHJlcXVlc3RIYW5kbGVyKHsgcmVxdWVzdCwgcGFnZSwgZW5xdWV1ZUxpbmtzLCBwdXNoRGF0YSwgbG9nIH0pIHtcXG4gICAgICAgIGNvbnN0IHRpdGxlID0gYXdhaXQgcGFnZS50aXRsZSgpO1xcbiAgICAgICAgbG9nLmluZm8oYFRpdGxlIG9mICR7cmVxdWVzdC5sb2FkZWRVcmx9IGlzICcke3RpdGxlfSdgKTtcXG5cXG4gICAgICAgIC8vIFNhdmUgcmVzdWx0cyBhcyBKU09OIHRvIC4vc3RvcmFnZS9kYXRhc2V0cy9kZWZhdWx0XFxuICAgICAgICBhd2FpdCBwdXNoRGF0YSh7IHRpdGxlLCB1cmw6IHJlcXVlc3QubG9hZGVkVXJsIH0pO1xcblxcbiAgICAgICAgLy8gRXh0cmFjdCBsaW5rcyBmcm9tIHRoZSBjdXJyZW50IHBhZ2VcXG4gICAgICAgIC8vIGFuZCBhZGQgdGhlbSB0byB0aGUgY3Jhd2xpbmcgcXVldWUuXFxuICAgICAgICBhd2FpdCBlbnF1ZXVlTGlua3MoKTtcXG4gICAgfSxcXG5cXG4gICAgLy8gVW5jb21tZW50IHRoaXMgb3B0aW9uIHRvIHNlZSB0aGUgYnJvd3NlciB3aW5kb3cuXFxuICAgIC8vIGhlYWRsZXNzOiBmYWxzZSxcXG5cXG4gICAgLy8gQ29tbWVudCB0aGlzIG9wdGlvbiB0byBzY3JhcGUgdGhlIGZ1bGwgd2Vic2l0ZS5cXG4gICAgbWF4UmVxdWVzdHNQZXJDcmF3bDogMjAsXFxufSk7XFxuXFxuLy8gQWRkIGZpcnN0IFVSTCB0byB0aGUgcXVldWUgYW5kIHN0YXJ0IHRoZSBjcmF3bC5cXG5hd2FpdCBjcmF3bGVyLnJ1bihbJ2h0dHBzOi8vY3Jhd2xlZS5kZXYnXSk7XFxuXFxuLy8gRXhwb3J0IHRoZSBlbnRpcmV0eSBvZiB0aGUgZGF0YXNldCB0byBhIHNpbmdsZSBmaWxlIGluXFxuLy8gLi9zdG9yYWdlL2tleV92YWx1ZV9zdG9yZXMvcmVzdWx0LmNzdlxcbmNvbnN0IGRhdGFzZXQgPSBhd2FpdCBjcmF3bGVyLmdldERhdGFzZXQoKTtcXG5hd2FpdCBkYXRhc2V0LmV4cG9ydFRvQ1NWKCdyZXN1bHQnKTtcXG5cXG4vLyBPciB3b3JrIHdpdGggdGhlIGRhdGEgZGlyZWN0bHkuXFxuY29uc3QgZGF0YSA9IGF3YWl0IGNyYXdsZXIuZ2V0RGF0YSgpO1xcbmNvbnNvbGUudGFibGUoZGF0YS5pdGVtcyk7XFxuXCJcbn0iLCJvcHRpb25zIjp7ImNvbnRlbnRUeXBlIjoiYXBwbGljYXRpb24vanNvbjsgY2hhcnNldD11dGYtOCIsIm1lbW9yeSI6NDA5Nn19.WKB14SjgTceKYyhONw2oXTkiOao6X4-UAS7cIuwqGvo&asrc=run_on_apify)\n\n```javascript\nimport { PlaywrightCrawler } from 'crawlee';// PlaywrightCrawler crawls the web using a headless browser controlled by the Playwright library.const crawler = new PlaywrightCrawler({ // Use the requestHandler to process each of the crawled pages. async requestHandler({ request, page, enqueueLinks, pushData, log }) { const title = await page.title(); log.info(`Title of ${request.loadedUrl} is '${title}'`); // Save results as JSON to `./storage/datasets/default` directory. await pushData({ title, url: request.loadedUrl }); // Extract links from the current page and add them to the crawling queue. await enqueueLinks(); }, // Uncomment this option to see the browser window. // headless: false, // Comment this option to scrape the full website. maxRequestsPerCrawl: 20,});// Add first URL to the queue and start the crawl.await crawler.run(['https://crawlee.dev']);// Export the whole dataset to a single file in `./result.csv`.await crawler.exportData('./result.csv');// Or work with the data directly.const data = await crawler.getData();console.table(data.items);\n```\n\nOr start with a template from our CLI\n\n` $npx crawlee create my-crawler `\n\nBuilt with  by Apify. Forever free and open-source.\n\n## Get started now!\n\nCrawlee wont fix broken selectors for you (yet), but it makes building and maintaining reliable crawlers faster and easierso you can focus on what matters most.",
			"chunks": [
				{
					"id": "home_chunk_1",
					"content": "## Build reliable web scrapers. Fast.\n\n\n\nCrawlee is a web scraping library for JavaScript and Python. It handles blocking, crawling, proxies, and browsers for you.\n\n![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-light.svg)![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-dark.svg)\n\n`npx crawlee create my-crawler`\n\n![Crawlee Python](https://crawlee.dev/img/crawlee-python-light.svg)![Crawlee Python](https://crawlee.dev/img/crawlee-python-dark.svg)\n\n`uvx 'crawlee[cli]' create my-crawler`",
					"tokensEstimate": 129,
					"sourceUrl": "https://crawlee.dev/",
					"pageTitle": "Crawlee for JavaScript 路 Build reliable crawlers. Fast."
				},
				{
					"id": "home_chunk_2",
					"content": "[Run on](https://console.apify.com/actors/6i5QsHBMtm3hKph70?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.eyJpbnB1dCI6IntcbiAgICBcImNvZGVcIjogXCJpbXBvcnQgeyBQbGF5d3JpZ2h0Q3Jhd2xlciB9IGZyb20gJ2NyYXdsZWUnO1xcblxcbi8vIENyYXdsZXIgc2V0dXAgZnJvbSB0aGUgcHJldmlvdXMgZXhhbXBsZS5cXG5jb25zdCBjcmF3bGVyID0gbmV3IFBsYXl3cmlnaHRDcmF3bGVyKHtcXG4gICAgLy8gVXNlIHRoZSByZXF1ZXN0SGFuZGxlciB0byBwcm9jZXNzIGVhY2ggb2YgdGhlIGNyYXdsZWQgcGFnZXMuXFxuICAgIGFzeW5jIHJlcXVlc3RIYW5kbGVyKHsgcmVxdWVzdCwgcGFnZSwgZW5xdWV1ZUxpbmtzLCBwdXNoRGF0YSwgbG9nIH0pIHtcXG4gICAgICAgIGNvbnN0IHRpdGxlID0gYXdhaXQgcGFnZS50aXRsZSgpO1xcbiAgICAgICAgbG9nLmluZm8oYFRpdGxlIG9mICR7cmVxdWVzdC5sb2FkZWRVcmx9IGlzICcke3RpdGxlfSdgKTtcXG5cXG4gICAgICAgIC8vIFNhdmUgcmVzdWx0cyBhcyBKU09OIHRvIC4vc3RvcmFnZS9kYXRhc2V0cy9kZWZhdWx0XFxuICAgICAgICBhd2FpdCBwdXNoRGF0YSh7IHRpdGxlLCB1cmw6IHJlcXVlc3QubG9hZGVkVXJsIH0pO1xcblxcbiAgICAgICAgLy8gRXh0cmFjdCBsaW5rcyBmcm9tIHRoZSBjdXJyZW50IHBhZ2VcXG4gICAgICAgIC8vIGFuZCBhZGQgdGhlbSB0byB0aGUgY3Jhd2xpbmcgcXVldWUuXFxuICAgICAgICBhd2FpdCBlbnF1ZXVlTGlua3MoKTtcXG4gICAgfSxcXG5cXG4gICAgLy8gVW5jb21tZW50IHRoaXMgb3B0aW9uIHRvIHNlZSB0aGUgYnJvd3NlciB3aW5kb3cuXFxuICAgIC8vIGhlYWRsZXNzOiBmYWxzZSxcXG5cXG4gICAgLy8gQ29tbWVudCB0aGlzIG9wdGlvbiB0byBzY3JhcGUgdGhlIGZ1bGwgd2Vic2l0ZS5cXG4gICAgbWF4UmVxdWVzdHNQZXJDcmF3bDogMjAsXFxufSk7XFxuXFxuLy8gQWRkIGZpcnN0IFVSTCB0byB0aGUgcXVldWUgYW5kIHN0YXJ0IHRoZSBjcmF3bC5cXG5hd2FpdCBjcmF3bGVyLnJ1bihbJ2h0dHBzOi8vY3Jhd2xlZS5kZXYnXSk7XFxuXFxuLy8gRXhwb3J0IHRoZSBlbnRpcmV0eSBvZiB0aGUgZGF0YXNldCB0byBhIHNpbmdsZSBmaWxlIGluXFxuLy8gLi9zdG9yYWdlL2tleV92YWx1ZV9zdG9yZXMvcmVzdWx0LmNzdlxcbmNvbnN0IGRhdGFzZXQgPSBhd2FpdCBjcmF3bGVyLmdldERhdGFzZXQoKTtcXG5hd2FpdCBkYXRhc2V0LmV4cG9ydFRvQ1NWKCdyZXN1bHQnKTtcXG5cXG4vLyBPciB3b3JrIHdpdGggdGhlIGRhdGEgZGlyZWN0bHkuXFxuY29uc3QgZGF0YSA9IGF3YWl0IGNyYXdsZXIuZ2V0RGF0YSgpO1xcbmNvbnNvbGUudGFibGUoZGF0YS5pdGVtcyk7XFxuXCJcbn0iLCJvcHRpb25zIjp7ImNvbnRlbnRUeXBlIjoiYXBwbGljYXRpb24vanNvbjsgY2hhcnNldD11dGYtOCIsIm1lbW9yeSI6NDA5Nn19.WKB14SjgTceKYyhONw2oXTkiOao6X4-UAS7cIuwqGvo&asrc=run_on_apify)",
					"tokensEstimate": 498,
					"sourceUrl": "https://crawlee.dev/",
					"pageTitle": "Crawlee for JavaScript 路 Build reliable crawlers. Fast."
				},
				{
					"id": "home_chunk_3",
					"content": "```javascript\nimport { PlaywrightCrawler } from 'crawlee';// PlaywrightCrawler crawls the web using a headless browser controlled by the Playwright library.const crawler = new PlaywrightCrawler({ // Use the requestHandler to process each of the crawled pages. async requestHandler({ request, page, enqueueLinks, pushData, log }) { const title = await page.title(); log.info(`Title of ${request.loadedUrl} is '${title}'`); // Save results as JSON to `./storage/datasets/default` directory. await pushData({ title, url: request.loadedUrl }); // Extract links from the current page and add them to the crawling queue. await enqueueLinks(); }, // Uncomment this option to see the browser window. // headless: false, // Comment this option to scrape the full website. maxRequestsPerCrawl: 20,});// Add first URL to the queue and start the crawl.await crawler.run(['https://crawlee.dev']);// Export the whole dataset to a single file in `./result.csv`.await crawler.exportData('./result.csv');// Or work with the data directly.const data = await crawler.getData();console.table(data.items);\n```\n\nOr start with a template from our CLI\n\n` $npx crawlee create my-crawler `\n\nBuilt with  by Apify. Forever free and open-source.\n\n## Get started now!\n\nCrawlee wont fix broken selectors for you (yet), but it makes building and maintaining reliable crawlers faster and easierso you can focus on what matters most.",
					"tokensEstimate": 350,
					"sourceUrl": "https://crawlee.dev/",
					"pageTitle": "Crawlee for JavaScript 路 Build reliable crawlers. Fast."
				}
			],
			"metadata": {
				"url": "https://crawlee.dev/",
				"title": "Crawlee for JavaScript 路 Build reliable crawlers. Fast.",
				"depth": 1,
				"crawledAt": "2026-02-02T05:20:12.219Z",
				"tokensEstimate": 977
			}
		},
		{
			"url": "https://crawlee.dev/js",
			"title": "Crawlee for JavaScript 路 Build reliable crawlers. Fast.",
			"markdown": "## Build reliable web scrapers. Fast.\n\nCrawlee is a web scraping library for JavaScript and Python. It handles blocking, crawling, proxies, and browsers for you.\n\n![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-light.svg)![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-dark.svg)\n\n[Run on](https://console.apify.com/actors/6i5QsHBMtm3hKph70?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.eyJpbnB1dCI6IntcbiAgICBcImNvZGVcIjogXCJpbXBvcnQgeyBQbGF5d3JpZ2h0Q3Jhd2xlciB9IGZyb20gJ2NyYXdsZWUnO1xcblxcbi8vIENyYXdsZXIgc2V0dXAgZnJvbSB0aGUgcHJldmlvdXMgZXhhbXBsZS5cXG5jb25zdCBjcmF3bGVyID0gbmV3IFBsYXl3cmlnaHRDcmF3bGVyKHtcXG4gICAgLy8gVXNlIHRoZSByZXF1ZXN0SGFuZGxlciB0byBwcm9jZXNzIGVhY2ggb2YgdGhlIGNyYXdsZWQgcGFnZXMuXFxuICAgIGFzeW5jIHJlcXVlc3RIYW5kbGVyKHsgcmVxdWVzdCwgcGFnZSwgZW5xdWV1ZUxpbmtzLCBwdXNoRGF0YSwgbG9nIH0pIHtcXG4gICAgICAgIGNvbnN0IHRpdGxlID0gYXdhaXQgcGFnZS50aXRsZSgpO1xcbiAgICAgICAgbG9nLmluZm8oYFRpdGxlIG9mICR7cmVxdWVzdC5sb2FkZWRVcmx9IGlzICcke3RpdGxlfSdgKTtcXG5cXG4gICAgICAgIC8vIFNhdmUgcmVzdWx0cyBhcyBKU09OIHRvIC4vc3RvcmFnZS9kYXRhc2V0cy9kZWZhdWx0XFxuICAgICAgICBhd2FpdCBwdXNoRGF0YSh7IHRpdGxlLCB1cmw6IHJlcXVlc3QubG9hZGVkVXJsIH0pO1xcblxcbiAgICAgICAgLy8gRXh0cmFjdCBsaW5rcyBmcm9tIHRoZSBjdXJyZW50IHBhZ2VcXG4gICAgICAgIC8vIGFuZCBhZGQgdGhlbSB0byB0aGUgY3Jhd2xpbmcgcXVldWUuXFxuICAgICAgICBhd2FpdCBlbnF1ZXVlTGlua3MoKTtcXG4gICAgfSxcXG5cXG4gICAgLy8gVW5jb21tZW50IHRoaXMgb3B0aW9uIHRvIHNlZSB0aGUgYnJvd3NlciB3aW5kb3cuXFxuICAgIC8vIGhlYWRsZXNzOiBmYWxzZSxcXG5cXG4gICAgLy8gQ29tbWVudCB0aGlzIG9wdGlvbiB0byBzY3JhcGUgdGhlIGZ1bGwgd2Vic2l0ZS5cXG4gICAgbWF4UmVxdWVzdHNQZXJDcmF3bDogMjAsXFxufSk7XFxuXFxuLy8gQWRkIGZpcnN0IFVSTCB0byB0aGUgcXVldWUgYW5kIHN0YXJ0IHRoZSBjcmF3bC5cXG5hd2FpdCBjcmF3bGVyLnJ1bihbJ2h0dHBzOi8vY3Jhd2xlZS5kZXYnXSk7XFxuXFxuLy8gRXhwb3J0IHRoZSBlbnRpcmV0eSBvZiB0aGUgZGF0YXNldCB0byBhIHNpbmdsZSBmaWxlIGluXFxuLy8gLi9zdG9yYWdlL2tleV92YWx1ZV9zdG9yZXMvcmVzdWx0LmNzdlxcbmNvbnN0IGRhdGFzZXQgPSBhd2FpdCBjcmF3bGVyLmdldERhdGFzZXQoKTtcXG5hd2FpdCBkYXRhc2V0LmV4cG9ydFRvQ1NWKCdyZXN1bHQnKTtcXG5cXG4vLyBPciB3b3JrIHdpdGggdGhlIGRhdGEgZGlyZWN0bHkuXFxuY29uc3QgZGF0YSA9IGF3YWl0IGNyYXdsZXIuZ2V0RGF0YSgpO1xcbmNvbnNvbGUudGFibGUoZGF0YS5pdGVtcyk7XFxuXCJcbn0iLCJvcHRpb25zIjp7ImNvbnRlbnRUeXBlIjoiYXBwbGljYXRpb24vanNvbjsgY2hhcnNldD11dGYtOCIsIm1lbW9yeSI6NDA5Nn19.WKB14SjgTceKYyhONw2oXTkiOao6X4-UAS7cIuwqGvo&asrc=run_on_apify)\n\n```javascript\nimport { PlaywrightCrawler } from 'crawlee';const crawler = new PlaywrightCrawler({ async requestHandler({ request, page, enqueueLinks, pushData, log }) { const title = await page.title(); log.info(`Title of ${request.loadedUrl} is '${title}'`); await pushData({ title, url: request.loadedUrl }); await enqueueLinks(); }, // Uncomment this option to see the browser window. // headless: false,});await crawler.run(['https://crawlee.dev']);\n```\n\nOr start with a template from our CLI\n\n` $npx crawlee create my-crawler `\n\nBuilt with  by Apify. Forever free and open-source.\n\n## What are the benefits?\n\n### Unblock websites by default\n\nCrawlee crawls stealthily with zero configuration, but you can customize its behavior to overcome any protection. Real-world fingerprints included.\n\n[Learn more](https://crawlee.dev/js/docs/guides/avoid-blocking)\n\n```text\n{ fingerprintOptions: { fingerprintGeneratorOptions: { browsers: ['chrome', 'firefox'], devices: ['mobile'], locales: ['en-US'], }, },},\n```\n\n### Work with your favorite tools\n\nCrawlee integrates BeautifulSoup, Cheerio, Puppeteer, Playwright, and other popular open-source tools. No need to learn new syntax.\n\n[Learn more](https://crawlee.dev/js/docs/quick-start#choose-your-crawler)\n\n![Work with your favorite tools](https://crawlee.dev/img/favorite-tools-light.webp)![Work with your favorite tools](https://crawlee.dev/img/favorite-tools-dark.webp)\n\n### One API for headless and HTTP\n\nSwitch between HTTP and headless without big rewrites thanks to a shared API. Or even let Adaptive crawler decide if JS rendering is needed.\n\n[Learn more](https://crawlee.dev/js/api/core)\n\n```javascript\nconst crawler = new AdaptivePlaywrightCrawler({ renderingTypeDetectionRatio: 0.1, async requestHandler({ querySelector, enqueueLinks }) { // The crawler detects if JS rendering is needed // to extract this data. If not, it will use HTTP // for follow-up requests to save time and costs. const $prices = await querySelector('span.price') await enqueueLinks(); },});\n```\n\n## What else is in Crawlee?\n\n## Deploy to cloud\n\nCrawlee, by Apify, works anywhere, but Apify offers the best experience. Easily turn your project into an\n\n[Actor](https://apify.com/actors)a serverless micro-app with built-in infra, proxies, and storage.\n\n[Deploy to Apify](https://crawlee.dev/js/docs/deployment/apify-platform)\n\nInstall Apify SDK and Apify CLI.\n\nAdd\n\nActor.init()\n\nto the begining and\n\nActor.exit()\n\nto the end of your code.\n\nUse the Apify CLI to push the code to the Apify platform.\n\n## Get started now!\n\nCrawlee wont fix broken selectors for you (yet), but it makes building and maintaining reliable crawlers faster and easierso you can focus on what matters most.",
			"chunks": [
				{
					"id": "js_chunk_1",
					"content": "## Build reliable web scrapers. Fast.\n\nCrawlee is a web scraping library for JavaScript and Python. It handles blocking, crawling, proxies, and browsers for you.\n\n![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-light.svg)![Crawlee JavaScript](https://crawlee.dev/img/crawlee-javascript-dark.svg)\n\n[Run on](https://console.apify.com/actors/6i5QsHBMtm3hKph70?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.eyJpbnB1dCI6IntcbiAgICBcImNvZGVcIjogXCJpbXBvcnQgeyBQbGF5d3JpZ2h0Q3Jhd2xlciB9IGZyb20gJ2NyYXdsZWUnO1xcblxcbi8vIENyYXdsZXIgc2V0dXAgZnJvbSB0aGUgcHJldmlvdXMgZXhhbXBsZS5cXG5jb25zdCBjcmF3bGVyID0gbmV3IFBsYXl3cmlnaHRDcmF3bGVyKHtcXG4gICAgLy8gVXNlIHRoZSByZXF1ZXN0SGFuZGxlciB0byBwcm9jZXNzIGVhY2ggb2YgdGhlIGNyYXdsZWQgcGFnZXMuXFxuICAgIGFzeW5jIHJlcXVlc3RIYW5kbGVyKHsgcmVxdWVzdCwgcGFnZSwgZW5xdWV1ZUxpbmtzLCBwdXNoRGF0YSwgbG9nIH0pIHtcXG4gICAgICAgIGNvbnN0IHRpdGxlID0gYXdhaXQgcGFnZS50aXRsZSgpO1xcbiAgICAgICAgbG9nLmluZm8oYFRpdGxlIG9mICR7cmVxdWVzdC5sb2FkZWRVcmx9IGlzICcke3RpdGxlfSdgKTtcXG5cXG4gICAgICAgIC8vIFNhdmUgcmVzdWx0cyBhcyBKU09OIHRvIC4vc3RvcmFnZS9kYXRhc2V0cy9kZWZhdWx0XFxuICAgICAgICBhd2FpdCBwdXNoRGF0YSh7IHRpdGxlLCB1cmw6IHJlcXVlc3QubG9hZGVkVXJsIH0pO1xcblxcbiAgICAgICAgLy8gRXh0cmFjdCBsaW5rcyBmcm9tIHRoZSBjdXJyZW50IHBhZ2VcXG4gICAgICAgIC8vIGFuZCBhZGQgdGhlbSB0byB0aGUgY3Jhd2xpbmcgcXVldWUuXFxuICAgICAgICBhd2FpdCBlbnF1ZXVlTGlua3MoKTtcXG4gICAgfSxcXG5cXG4gICAgLy8gVW5jb21tZW50IHRoaXMgb3B0aW9uIHRvIHNlZSB0aGUgYnJvd3NlciB3aW5kb3cuXFxuICAgIC8vIGhlYWRsZXNzOiBmYWxzZSxcXG5cXG4gICAgLy8gQ29tbWVudCB0aGlzIG9wdGlvbiB0byBzY3JhcGUgdGhlIGZ1bGwgd2Vic2l0ZS5cXG4gICAgbWF4UmVxdWVzdHNQZXJDcmF3bDogMjAsXFxufSk7XFxuXFxuLy8gQWRkIGZpcnN0IFVSTCB0byB0aGUgcXVldWUgYW5kIHN0YXJ0IHRoZSBjcmF3bC5cXG5hd2FpdCBjcmF3bGVyLnJ1bihbJ2h0dHBzOi8vY3Jhd2xlZS5kZXYnXSk7XFxuXFxuLy8gRXhwb3J0IHRoZSBlbnRpcmV0eSBvZiB0aGUgZGF0YXNldCB0byBhIHNpbmdsZSBmaWxlIGluXFxuLy8gLi9zdG9yYWdlL2tleV92YWx1ZV9zdG9yZXMvcmVzdWx0LmNzdlxcbmNvbnN0IGRhdGFzZXQgPSBhd2FpdCBjcmF3bGVyLmdldERhdGFzZXQoKTtcXG5hd2FpdCBkYXRhc2V0LmV4cG9ydFRvQ1NWKCdyZXN1bHQnKTtcXG5cXG4vLyBPciB3b3JrIHdpdGggdGhlIGRhdGEgZGlyZWN0bHkuXFxuY29uc3QgZGF0YSA9IGF3YWl0IGNyYXdsZXIuZ2V0RGF0YSgpO1xcbmNvbnNvbGUudGFibGUoZGF0YS5pdGVtcyk7XFxuXCJcbn0iLCJvcHRpb25zIjp7ImNvbnRlbnRUeXBlIjoiYXBwbGljYXRpb24vanNvbjsgY2hhcnNldD11dGYtOCIsIm1lbW9yeSI6NDA5Nn19.WKB14SjgTceKYyhONw2oXTkiOao6X4-UAS7cIuwqGvo&asrc=run_on_apify)\n\n```javascript\nimport { PlaywrightCrawler } from 'crawlee';const crawler = new PlaywrightCrawler({ async requestHandler({ request, page, enqueueLinks, pushData, log }) { const title = await page.title(); log.info(`Title of ${request.loadedUrl} is '${title}'`); await pushData({ title, url: request.loadedUrl }); await enqueueLinks(); }, // Uncomment this option to see the browser window. // headless: false,});await crawler.run(['https://crawlee.dev']);\n```\n\nOr start with a template from our CLI\n\n` $npx crawlee create my-crawler `\n\nBuilt with  by Apify. Forever free and open-source.\n\n## What are the benefits?\n\n### Unblock websites by default\n\nCrawlee crawls stealthily with zero configuration, but you can customize its behavior to overcome any protection. Real-world fingerprints included.\n\n[Learn more](https://crawlee.dev/js/docs/guides/avoid-blocking)\n\n```text\n{ fingerprintOptions: { fingerprintGeneratorOptions: { browsers: ['chrome', 'firefox'], devices: ['mobile'], locales: ['en-US'], }, },},\n```",
					"tokensEstimate": 827,
					"sourceUrl": "https://crawlee.dev/js",
					"pageTitle": "Crawlee for JavaScript 路 Build reliable crawlers. Fast."
				},
				{
					"id": "js_chunk_2",
					"content": "### Work with your favorite tools\n\nCrawlee integrates BeautifulSoup, Cheerio, Puppeteer, Playwright, and other popular open-source tools. No need to learn new syntax.\n\n[Learn more](https://crawlee.dev/js/docs/quick-start#choose-your-crawler)\n\n![Work with your favorite tools](https://crawlee.dev/img/favorite-tools-light.webp)![Work with your favorite tools](https://crawlee.dev/img/favorite-tools-dark.webp)",
					"tokensEstimate": 102,
					"sourceUrl": "https://crawlee.dev/js",
					"pageTitle": "Crawlee for JavaScript 路 Build reliable crawlers. Fast."
				},
				{
					"id": "js_chunk_3",
					"content": "### One API for headless and HTTP\n\nSwitch between HTTP and headless without big rewrites thanks to a shared API. Or even let Adaptive crawler decide if JS rendering is needed.\n\n[Learn more](https://crawlee.dev/js/api/core)\n\n```javascript\nconst crawler = new AdaptivePlaywrightCrawler({ renderingTypeDetectionRatio: 0.1, async requestHandler({ querySelector, enqueueLinks }) { // The crawler detects if JS rendering is needed // to extract this data. If not, it will use HTTP // for follow-up requests to save time and costs. const $prices = await querySelector('span.price') await enqueueLinks(); },});\n```\n\n## What else is in Crawlee?",
					"tokensEstimate": 158,
					"sourceUrl": "https://crawlee.dev/js",
					"pageTitle": "Crawlee for JavaScript 路 Build reliable crawlers. Fast."
				},
				{
					"id": "js_chunk_4",
					"content": "## Deploy to cloud\n\nCrawlee, by Apify, works anywhere, but Apify offers the best experience. Easily turn your project into an\n\n[Actor](https://apify.com/actors)a serverless micro-app with built-in infra, proxies, and storage.\n\n[Deploy to Apify](https://crawlee.dev/js/docs/deployment/apify-platform)\n\nInstall Apify SDK and Apify CLI.\n\nAdd\n\nActor.init()\n\nto the begining and\n\nActor.exit()\n\nto the end of your code.\n\nUse the Apify CLI to push the code to the Apify platform.\n\n## Get started now!\n\nCrawlee wont fix broken selectors for you (yet), but it makes building and maintaining reliable crawlers faster and easierso you can focus on what matters most.",
					"tokensEstimate": 162,
					"sourceUrl": "https://crawlee.dev/js",
					"pageTitle": "Crawlee for JavaScript 路 Build reliable crawlers. Fast."
				}
			],
			"metadata": {
				"url": "https://crawlee.dev/js",
				"title": "Crawlee for JavaScript 路 Build reliable crawlers. Fast.",
				"depth": 1,
				"crawledAt": "2026-02-02T05:20:12.438Z",
				"tokensEstimate": 1248
			}
		},
		{
			"url": "https://crawlee.dev/js/docs/quick-start",
			"title": "Quick Start",
			"markdown": "- - Quick Start\n\nVersion: 3.15\n\n## Quick Start\n\nWith this short tutorial you can start scraping with Crawlee in a minute or two. To learn in-depth how Crawlee works, read the [Introduction](/js/docs/introduction), which is a comprehensive step-by-step guide for creating your first scraper.\n\n### Choose your crawler\n\nCrawlee comes with three main crawler classes: [`CheerioCrawler`](/js/api/cheerio-crawler/class/CheerioCrawler), [`PuppeteerCrawler`](/js/api/puppeteer-crawler/class/PuppeteerCrawler) and [`PlaywrightCrawler`](/js/api/playwright-crawler/class/PlaywrightCrawler). All classes share the same interface for maximum flexibility when switching between them.\n\n#### CheerioCrawler\n\nThis is a plain HTTP crawler. It parses HTML using the [Cheerio](https://github.com/cheeriojs/cheerio) library and crawls the web using the specialized [got-scraping](https://github.com/apify/got-scraping) HTTP client which masks as a browser. It's very fast and efficient, but can't handle JavaScript rendering.\n\n#### PuppeteerCrawler\n\nThis crawler uses a headless browser to crawl, controlled by the [Puppeteer](https://github.com/puppeteer/puppeteer) library. It can control Chromium or Chrome. Puppeteer is the de-facto standard in headless browser automation.\n\n#### PlaywrightCrawler\n\n[Playwright](https://github.com/microsoft/playwright) is a more powerful and full-featured successor to Puppeteer. It can control Chromium, Chrome, Firefox, Webkit and many other browsers. If you're not familiar with Puppeteer already, and you need a headless browser, go with Playwright.\n\nbefore you start\n\nCrawlee requires [Node.js 16 or later](https://nodejs.org/en/).\n\n### Installation with Crawlee CLI\n\nThe fastest way to try Crawlee out is to use the **Crawlee CLI** and choose the **Getting started example**. The CLI will install all the necessary dependencies and add boilerplate code for you to play with.\n\n```bash\nnpx crawlee create my-crawler\n```\n\nAfter the installation is complete you can start the crawler like this:\n\n```bash\ncd my-crawler &&npm start\n```\n\n### Manual installation\n\nYou can add Crawlee to any Node.js project by running:\n\n- CheerioCrawler- PlaywrightCrawler- PuppeteerCrawler\n\n```bash\nnpminstall crawlee\n```\n\n### Crawling\n\nRun the following example to perform a recursive crawl of the Crawlee website using the selected crawler.\n\nDon't forget about module imports\n\nTo run the example, add a `\"type\": \"module\"` clause into your `package.json` or copy it into a file with an `.mjs` suffix. This enables `import` statements in Node.js. See [Node.js docs](https://nodejs.org/dist/latest-v16.x/docs/api/esm.html#enabling) for more information.\n\n- CheerioCrawler- PlaywrightCrawler- PuppeteerCrawler\n\n```block\nRun onimport{ CheerioCrawler, Dataset }from'crawlee';// CheerioCrawler crawls the web using HTTP requests// and parses HTML using the Cheerio library.const crawler =newCheerioCrawler({// Use the requestHandler to process each of the crawled pages.asyncrequestHandler({ request, $, enqueueLinks, log }){const title =$('title').text(); log.info(`Title of ${request.loadedUrl} is '${title}'`);// Save results as JSON to ./storage/datasets/defaultawait Dataset.pushData({ title, url: request.loadedUrl });// Extract links from the current page// and add them to the crawling queue.awaitenqueueLinks();},// Let's limit our crawls to make our tests shorter and safer. maxRequestsPerCrawl:50,});// Add first URL to the queue and start the crawl.await crawler.run(['https://crawlee.dev']);\n```\n\nWhen you run the example, you will see Crawlee automating the data extraction process in your terminal.\n\n```log\nINFOCheerioCrawler: Starting the crawlINFOCheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee 路 Build reliable crawlers. Fast. | Crawlee'INFOCheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'INFOCheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'INFOCheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```\n\n#### Running headful browsers\n\nBrowsers controlled by Puppeteer and Playwright run headless (without a visible window). You can switch to headful by adding the `headless: false` option to the crawlers' constructor. This is useful in the development phase when you want to see what's going on in the browser.\n\n- PlaywrightCrawler- PuppeteerCrawler\n\n```block\nRun onimport{ PlaywrightCrawler, Dataset }from'crawlee';const crawler =newPlaywrightCrawler({asyncrequestHandler({ request, page, enqueueLinks, log }){const title =await page.title(); log.info(`Title of ${request.loadedUrl} is '${title}'`);await Dataset.pushData({ title, url: request.loadedUrl });awaitenqueueLinks();},// When you turn off headless mode, the crawler// will run with a visible browser window. headless:false,// Let's limit our crawls to make our tests shorter and safer. maxRequestsPerCrawl:50,});// Add first URL to the queue and start the crawl.await crawler.run(['https://crawlee.dev']);\n```\n\nWhen you run the example code, you'll see an automated browser blaze through the Crawlee website.\n\nnote\n\nFor the sake of this show off, we've slowed down the crawler, but rest assured, it's blazing fast in real world usage.\n\n![An image showing off Crawlee scraping the Crawlee website using Puppeteer/Playwright and Chromium](/img/chrome-scrape-light.gif)![An image showing off Crawlee scraping the Crawlee website using Puppeteer/Playwright and Chromium](/img/chrome-scrape-dark.gif)\n\n### Results\n\nCrawlee stores data to the `./storage` directory in your current working directory. The results of your crawl will be available under `./storage/datasets/default/*.json` as JSON files.\n\n```json\n./storage/datasets/default/000000001.json{\"url\":\"https://crawlee.dev/\",\"title\":\"Crawlee 路 The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"}\n```\n\ntip\n\nYou can override the storage directory by setting the `CRAWLEE_STORAGE_DIR` environment variable.\n\n### Examples and further reading\n\nYou can find more examples showcasing various features of Crawlee in the [Examples](/js/docs/examples) section of the documentation. To better understand Crawlee and its components you should read the [Introduction](/js/docs/introduction) step-by-step guide.\n\n**Related links**\n\n- [Configuration](/js/docs/guides/configuration)\n- [Request storage](/js/docs/guides/request-storage)\n- [Result storage](/js/docs/guides/result-storage)\n\n[Edit this page](https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.15/quick-start/index.mdx)\n\nLast updated on **Jan 30, 2026** by **nikitachapovskii-dev**\n\n[Next\n\nIntroduction](/js/docs/introduction)",
			"chunks": [
				{
					"id": "js_docs_quick_start_chunk_1",
					"content": "- - Quick Start\n\nVersion: 3.15\n\n## Quick Start\n\nWith this short tutorial you can start scraping with Crawlee in a minute or two. To learn in-depth how Crawlee works, read the [Introduction](/js/docs/introduction), which is a comprehensive step-by-step guide for creating your first scraper.\n\n### Choose your crawler\n\nCrawlee comes with three main crawler classes: [`CheerioCrawler`](/js/api/cheerio-crawler/class/CheerioCrawler), [`PuppeteerCrawler`](/js/api/puppeteer-crawler/class/PuppeteerCrawler) and [`PlaywrightCrawler`](/js/api/playwright-crawler/class/PlaywrightCrawler). All classes share the same interface for maximum flexibility when switching between them.\n\n#### CheerioCrawler\n\nThis is a plain HTTP crawler. It parses HTML using the [Cheerio](https://github.com/cheeriojs/cheerio) library and crawls the web using the specialized [got-scraping](https://github.com/apify/got-scraping) HTTP client which masks as a browser. It's very fast and efficient, but can't handle JavaScript rendering.\n\n#### PuppeteerCrawler\n\nThis crawler uses a headless browser to crawl, controlled by the [Puppeteer](https://github.com/puppeteer/puppeteer) library. It can control Chromium or Chrome. Puppeteer is the de-facto standard in headless browser automation.\n\n#### PlaywrightCrawler\n\n[Playwright](https://github.com/microsoft/playwright) is a more powerful and full-featured successor to Puppeteer. It can control Chromium, Chrome, Firefox, Webkit and many other browsers. If you're not familiar with Puppeteer already, and you need a headless browser, go with Playwright.\n\nbefore you start\n\nCrawlee requires [Node.js 16 or later](https://nodejs.org/en/).\n\n### Installation with Crawlee CLI\n\nThe fastest way to try Crawlee out is to use the **Crawlee CLI** and choose the **Getting started example**. The CLI will install all the necessary dependencies and add boilerplate code for you to play with.\n\n```bash\nnpx crawlee create my-crawler\n```\n\nAfter the installation is complete you can start the crawler like this:\n\n```bash\ncd my-crawler &&npm start\n```\n\n### Manual installation\n\nYou can add Crawlee to any Node.js project by running:\n\n- CheerioCrawler- PlaywrightCrawler- PuppeteerCrawler\n\n```bash\nnpminstall crawlee\n```",
					"tokensEstimate": 551,
					"sourceUrl": "https://crawlee.dev/js/docs/quick-start",
					"pageTitle": "Quick Start"
				},
				{
					"id": "js_docs_quick_start_chunk_2",
					"content": "### Crawling\n\nRun the following example to perform a recursive crawl of the Crawlee website using the selected crawler.\n\nDon't forget about module imports\n\nTo run the example, add a `\"type\": \"module\"` clause into your `package.json` or copy it into a file with an `.mjs` suffix. This enables `import` statements in Node.js. See [Node.js docs](https://nodejs.org/dist/latest-v16.x/docs/api/esm.html#enabling) for more information.\n\n- CheerioCrawler- PlaywrightCrawler- PuppeteerCrawler\n\n```block\nRun onimport{ CheerioCrawler, Dataset }from'crawlee';// CheerioCrawler crawls the web using HTTP requests// and parses HTML using the Cheerio library.const crawler =newCheerioCrawler({// Use the requestHandler to process each of the crawled pages.asyncrequestHandler({ request, $, enqueueLinks, log }){const title =$('title').text(); log.info(`Title of ${request.loadedUrl} is '${title}'`);// Save results as JSON to ./storage/datasets/defaultawait Dataset.pushData({ title, url: request.loadedUrl });// Extract links from the current page// and add them to the crawling queue.awaitenqueueLinks();},// Let's limit our crawls to make our tests shorter and safer. maxRequestsPerCrawl:50,});// Add first URL to the queue and start the crawl.await crawler.run(['https://crawlee.dev']);\n```\n\nWhen you run the example, you will see Crawlee automating the data extraction process in your terminal.\n\n```log\nINFOCheerioCrawler: Starting the crawlINFOCheerioCrawler: Title of https://crawlee.dev/ is 'Crawlee 路 Build reliable crawlers. Fast. | Crawlee'INFOCheerioCrawler: Title of https://crawlee.dev/js/docs/examples is 'Examples | Crawlee'INFOCheerioCrawler: Title of https://crawlee.dev/js/docs/quick-start is 'Quick Start | Crawlee'INFOCheerioCrawler: Title of https://crawlee.dev/js/docs/guides is 'Guides | Crawlee'\n```",
					"tokensEstimate": 451,
					"sourceUrl": "https://crawlee.dev/js/docs/quick-start",
					"pageTitle": "Quick Start"
				},
				{
					"id": "js_docs_quick_start_chunk_3",
					"content": "#### Running headful browsers\n\nBrowsers controlled by Puppeteer and Playwright run headless (without a visible window). You can switch to headful by adding the `headless: false` option to the crawlers' constructor. This is useful in the development phase when you want to see what's going on in the browser.\n\n- PlaywrightCrawler- PuppeteerCrawler\n\n```block\nRun onimport{ PlaywrightCrawler, Dataset }from'crawlee';const crawler =newPlaywrightCrawler({asyncrequestHandler({ request, page, enqueueLinks, log }){const title =await page.title(); log.info(`Title of ${request.loadedUrl} is '${title}'`);await Dataset.pushData({ title, url: request.loadedUrl });awaitenqueueLinks();},// When you turn off headless mode, the crawler// will run with a visible browser window. headless:false,// Let's limit our crawls to make our tests shorter and safer. maxRequestsPerCrawl:50,});// Add first URL to the queue and start the crawl.await crawler.run(['https://crawlee.dev']);\n```\n\nWhen you run the example code, you'll see an automated browser blaze through the Crawlee website.\n\nnote\n\nFor the sake of this show off, we've slowed down the crawler, but rest assured, it's blazing fast in real world usage.\n\n![An image showing off Crawlee scraping the Crawlee website using Puppeteer/Playwright and Chromium](/img/chrome-scrape-light.gif)![An image showing off Crawlee scraping the Crawlee website using Puppeteer/Playwright and Chromium](/img/chrome-scrape-dark.gif)",
					"tokensEstimate": 362,
					"sourceUrl": "https://crawlee.dev/js/docs/quick-start",
					"pageTitle": "Quick Start"
				},
				{
					"id": "js_docs_quick_start_chunk_4",
					"content": "### Results\n\nCrawlee stores data to the `./storage` directory in your current working directory. The results of your crawl will be available under `./storage/datasets/default/*.json` as JSON files.\n\n```json\n./storage/datasets/default/000000001.json{\"url\":\"https://crawlee.dev/\",\"title\":\"Crawlee 路 The scalable web crawling, scraping and automation library for JavaScript/Node.js | Crawlee\"}\n```\n\ntip\n\nYou can override the storage directory by setting the `CRAWLEE_STORAGE_DIR` environment variable.",
					"tokensEstimate": 124,
					"sourceUrl": "https://crawlee.dev/js/docs/quick-start",
					"pageTitle": "Quick Start"
				},
				{
					"id": "js_docs_quick_start_chunk_5",
					"content": "### Examples and further reading\n\nYou can find more examples showcasing various features of Crawlee in the [Examples](/js/docs/examples) section of the documentation. To better understand Crawlee and its components you should read the [Introduction](/js/docs/introduction) step-by-step guide.\n\n**Related links**\n\n- [Configuration](/js/docs/guides/configuration)\n- [Request storage](/js/docs/guides/request-storage)\n- [Result storage](/js/docs/guides/result-storage)\n\n[Edit this page](https://github.com/apify/crawlee/edit/master/website/versioned_docs/version-3.15/quick-start/index.mdx)\n\nLast updated on **Jan 30, 2026** by **nikitachapovskii-dev**\n\n[Next\n\nIntroduction](/js/docs/introduction)",
					"tokensEstimate": 172,
					"sourceUrl": "https://crawlee.dev/js/docs/quick-start",
					"pageTitle": "Quick Start"
				}
			],
			"metadata": {
				"url": "https://crawlee.dev/js/docs/quick-start",
				"title": "Quick Start",
				"depth": 1,
				"crawledAt": "2026-02-02T05:20:12.780Z",
				"tokensEstimate": 1661
			}
		}
	]
}